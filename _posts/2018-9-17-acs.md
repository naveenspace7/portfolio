---
layout: post
title: Advanced Computing Systems
excerpt_separator: <!--more-->
---

<!--more-->

Tools used: C++, C++ STL, GProf, GNU Compiler, CMake, OpenMP, OpenCL, CUDA, NVProf

Project source: 

Contributing member: 

## Task: Matrix Multiplication three implementation:

The goal of this task is to explore different parallel programming techniques and benchmark their speed up. We use vector inner product and traditional matrix multiplication method to perform the benchmarks in three different constructs:

1. Single Instruction Multiple Data (SIMD)
2. OpenMP
3. OpenCL

### Benchmark details

The time consumed shown in this section is a result of running each trail three times. The benchmarking is done on datatypes double and float.

#### Vector Inner Product

For vectors of dimensions A(1xN) and B(Nx1), the vector product consists of N multiplication and N-1 addition operations. Figure . shows the increase in time with the increase in size for both the datatypes.

<!-- show matrix equation here -->

<!-- show baseline for vector multiplication -->

#### Matrix Multiplication

For given matrices A and B, serial method ses multiple loops to fetch the right elements from each matrix and performs element-wise multiplication. In addition to that, it uses another loop to perform addition to calculate an element in the result. For two matrices A (MxK) and B (KxN), to obtain the result C (MxN), it performs M.N.K number of multiplications and M.N.(K-1) number of additions.

<!-- show baseline for matrix multiplication -->

A conventional serial implementation can benefit from parallelism. The subsequent section shows how the computation time is decreased with each of the parallel constructs.

### SIMD Extensions

SIMD extensions are supported on Intel and ARM in the form of AVX and Neon respectively. In this discussion we stick to Intel's AVX (Advanced Vector eXtensions). 

As the name suggests, SIMD can perform an arithmetic operation on multiple data in a single clock cycle. Depending on the AVX version present in the processor, the size of the AVX registers vary. For the experiments performed in this post, AVX-256 version, in which the registers are 256 bits, is used. The number of data elements that fit into the register depends on the data type. For example, 8 float elements or 4 double elements can fit into an AVX-256 register.

The following are the steps taken:

1. To achieve a speed up, the rows of the first matrix and columns of the second matrix are loaded into two separate AVX registers and use SIMD instructions to perform multiplication. This step is a major speed up in this technique. While the multiplication got faster with the implementation of SIMD, the horizontal addition would still remain the bottleneck in the performance. This step alone didn't account for a major speed-up.

2. After profiling further, it was found that the time taken to load the data into the registers is one of the bottleneck. The row of first matrix is loaded into the first vector and remain until the all the columns are multiplied. By doing this, a significant improvement was observed. 

3. To improve upon this furhter, we have identified another bottleneck which is to be the horizontal additions. This was again achieved with vector horizontal additions. This replaces another step in the iteration.

The final benchmarks are shown below put Figure 8.

![AVX Benchmarks]({{site.url}}/portfolio/assets/images/avx_bench.png)

### OpenMP

OpenMP is a very simple and practical technique to implement parallelism to a program. It instructs a given number of threads to work in parallel on a specified part of a program. This is known as multithreading, where the master thread forks off into multiple threads to execute that particular piece in parallel [todo-ref]. This method yields faster run times compared to serial executions of the same task. OpenMP commands are invoked by `pragma`s in the code and a clause. The implementation of OpenMP uses `for` loops which perform matrix multiplication analytically. That is, the outer most loop keeps track of the row of the A matrix, the middle loop tracks the columns of matrix B, and the inner most goes through the current column of A and current row of B. With some basic math we are able to compute the resulting matrix. Parallelizing the for-loop can be done with the following pragma:

`#pragma omp for`

We immediately saw the performance increase of using OpenMP compared withe the baseline matrix multiplication. Additionally, it was noticed that the difference between using 4 and 8 threads were slim for low sized matrices. However, for bigger matrices the performance of using 8 threads was significantly greater. 

![OMP Float Benchmarks]({{site.url}}/portfolio/assets/images/omp_float.png)

![OMP Double Benchmarks]({{site.url}}/portfolio/assets/images/omp_double.png)

The results obtained don't adhere to Amdhal's law mainly because of the cache misses when the work is split among multiple cores.

Verdict:
<!-- SIMD -->
As it can be observed, using SIMD doesn't give the highest performance gain. Despite that being the case, SIMD neither needs multiple cores nor any expensive hardware additions. SIMD takes the advantage of the ISA.

<!-- OpenMP -->